{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Speech Recognition\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import cv2\n",
    "import pickle\n",
    "\n",
    "import librosa\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, TensorBoard"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.set()\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"expand_frame_repr\", False)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from clean_audio import CleanAudio\n",
    "from file_handler import FileHandler\n",
    "from audio_vis import AudioVis\n",
    "from log_melgram_layer import LogMelgramLayer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clean_audio = CleanAudio()\n",
    "file_handler = FileHandler()\n",
    "audio_vis = AudioVis()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PATH_TRAIN_WAV = \"../data/AMHARIC_CLEAN/train/wav/\"\n",
    "PATH_TEST_WAV = \"../data/AMHARIC_CLEAN/test/wav/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(r'../data/clean_data.csv')\n",
    "data.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_paths(df):\n",
    "  paths = []\n",
    "  for col, row in df.iterrows():\n",
    "    if(row[\"category\"] == \"Train\"):\n",
    "      paths.append(PATH_TRAIN_WAV + row[\"key\"] + \".npy\")\n",
    "    else:\n",
    "      paths.append(PATH_TEST_WAV + row[\"key\"] + \".npy\")\n",
    "\n",
    "  return paths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[\"path\"] = get_paths(data)\n",
    "data.sort_values(by=[\"duration\"], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data = data[[\"text\", \"char_length\", \"duration\", \"path\"]]\n",
    "data[[\"text\", \"char_length\", \"duration\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    def __init__(self, texts, padding, len_sent, filters, reverse=False):\n",
    "        Tokenizer.__init__(self, filters=filters, char_level=True)\n",
    "\n",
    "        self.len_sent = len_sent\n",
    "        self.fit_on_texts(texts)\n",
    "\n",
    "        self.index_to_word = dict(zip(self.word_index.values(), self.word_index.keys()))\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "\n",
    "        if reverse:\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen=len_sent,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating\n",
    "                                           )\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word\n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        words = [self.index_to_word[token] for token in tokens if token != 0]\n",
    "        text = \"\".join(words)\n",
    "        return text\n",
    "\n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "\n",
    "        if padding:\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen=self.len_sent,\n",
    "                                   padding=truncating,\n",
    "                                   truncating=truncating\n",
    "                                   )\n",
    "        return tokens\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "MAX_SENTENCE_LENGTH = 125       # The longest sentence in the data is around 150 chars\n",
    "filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n።”፤፦’፥'  # { ።”፤፦’፥' } unique for amharic"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "tokenizer = TokenizerWrap(texts=data.text,\n",
    "                          padding='post',\n",
    "                          reverse=False,\n",
    "                          len_sent=MAX_SENTENCE_LENGTH,\n",
    "                          filters=filters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(tokenizer.word_index))\n",
    "print(tokenizer.word_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.text[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample = tokenizer.text_to_tokens(data.text[1], padding=True)\n",
    "sample"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(tokenizer.tokens_to_string(sample[0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "save token"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('../models/char_tokenizer_amharic.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Augmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AudioAugment():\n",
    "  def __init__(self):\n",
    "    pass\n",
    "   \n",
    "  def change_speed(self, data):\n",
    "    speed_rate = np.random.uniform(0.8, 1.2)\n",
    "    wav_speed_tune = cv2.resize(data, (1, int(len(data) * speed_rate))).squeeze()\n",
    "\n",
    "    if len(wav_speed_tune) < len(data):\n",
    "      padding = len(data) - len(wav_speed_tune)\n",
    "      offset = padding // 2\n",
    "      wav_speed_tune = np.pad(wav_speed_tune, (offset, padding - offset), \"constant\")\n",
    "    else:\n",
    "      wav_speed_tune = wav_speed_tune[:len(data)]\n",
    "\n",
    "    return wav_speed_tune\n",
    "\n",
    "  def add_noise(self, data, noise_levels=(0, 0.3)):\n",
    "    noise_level = np.random.uniform(*noise_levels)\n",
    "    noise = np.random.randn(len(data))\n",
    "    data_noise = data + noise_level * noise\n",
    "\n",
    "    return data_noise\n",
    "\n",
    "  def change_pitch(self, data):\n",
    "    n_steps = np.random.randint(-1, 2)\n",
    "    return librosa.effects.pitch_shift(data, 8000, n_steps)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataGenerator\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, sr, batch_size=32, shuffle=True):\n",
    "        self.data = data      # Data Augmentation\n",
    "        self.sr = sr\n",
    "        self.batch_size = batch_size / 4      # Data Augmentation\n",
    "        self.audio_augment = AudioAugment()\n",
    "        self.len = int(np.floor(data.shape[0]/ self.batch_size))\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __data_generation(self, batch_data):\n",
    "\n",
    "        longest_audio = int(batch_data[\"duration\"].max() * self.sr)\n",
    "        longest_trans = int(batch_data[\"char_length\"].max())\n",
    "\n",
    "        X_audio = np.zeros([int(self.batch_size * 4), longest_audio], dtype=\"float32\")\n",
    "        y_trans = np.ones([int(self.batch_size * 4), longest_trans], dtype=\"int64\")\n",
    "        X_length = np.ones([int(self.batch_size * 4), 1], dtype=\"int64\") * longest_audio\n",
    "        y_length = np.zeros([int(self.batch_size * 4), 1], dtype=\"int64\")\n",
    "\n",
    "        i = 0\n",
    "        for col, row in batch_data.iterrows():\n",
    "\n",
    "            # Add transcription\n",
    "            transcription = tf.convert_to_tensor(tokenizer.text_to_tokens(row[\"text\"], padding=True)[:, :longest_trans])\n",
    "            y_trans[i,] = y_trans[i + 1,] = y_trans[i + 2,] = y_trans[i + 3,] = transcription\n",
    "            y_length[i] = y_length[i + 1] = y_length[i + 2] = y_length[i + 3] = row[\"char_length\"]\n",
    "\n",
    "            # Add original Audio\n",
    "            wav = np.load(row[\"path\"])\n",
    "            audio_length = int(row[\"duration\"] * self.sr)\n",
    "            X_audio[i, :audio_length] = wav\n",
    "            i += 1\n",
    "\n",
    "            # Add noise\n",
    "            wav_ = self.audio_augment.add_noise(wav)\n",
    "            X_audio[i, :audio_length] = wav_\n",
    "            i += 1\n",
    "\n",
    "            # Add noise\n",
    "            wav_ = self.audio_augment.add_noise(wav)\n",
    "            X_audio[i, :audio_length] = wav_\n",
    "            i += 1\n",
    "\n",
    "            # # Pitch change\n",
    "            # wav_ = self.audio_augment.change_pitch(wav)\n",
    "            # X_audio[i, :audio_length] = wav_\n",
    "            # i+=1\n",
    "\n",
    "            # Speed change\n",
    "            wav_ = self.audio_augment.change_speed(wav)\n",
    "            X_audio[i, :audio_length] = wav_\n",
    "            i += 1\n",
    "\n",
    "        outputs = {'ctc': tf.zeros(([int(self.batch_size * 4)]), dtype=tf.dtypes.float32)}\n",
    "        inputs = {\n",
    "            'the_input': tf.convert_to_tensor(X_audio),\n",
    "            'the_labels': tf.convert_to_tensor(y_trans),\n",
    "            'input_length': tf.convert_to_tensor(X_length, dtype=\"float32\"),\n",
    "            'label_length': tf.convert_to_tensor(y_length)\n",
    "        }\n",
    "        return (inputs, outputs)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        self.indexes = np.arange(self.len * self.batch_size)\n",
    "\n",
    "        if self.shuffle == True:\n",
    "\n",
    "            self.indexes = self.indexes.reshape(int(self.len), int(self.batch_size))\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            for i in range(self.len):\n",
    "                np.random.shuffle(self.indexes[i])\n",
    "\n",
    "            self.indexes = self.indexes.reshape(int(self.len * self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[int(index * self.batch_size):int((index + 1) * self.batch_size)]\n",
    "        batch_data = self.data.iloc[indexes]\n",
    "        return self.__data_generation(batch_data)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sr = 8000\n",
    "batch_size = 128\n",
    "sample_generator = DataGenerator(data, sr, batch_size, False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_generator.__len__()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "sample_data = sample_generator.__getitem__(261)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_audios = sample_data[0][\"the_input\"]\n",
    "sample_labels = sample_data[0][\"the_labels\"]\n",
    "sample_audios_length = sample_data[0][\"input_length\"]\n",
    "sample_labels_length = sample_data[0][\"label_length\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(sample_audios.shape)\n",
    "print(sample_labels.shape)\n",
    "print(sample_audios_length.shape)\n",
    "print(sample_labels_length.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_labels[0]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(tokenizer.tokens_to_string(sample_labels[0].numpy()))\n",
    "audio_vis.play_audio(sample_audios[0], sr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hkfhghj"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Log Melgram\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocessin_model(fft_size, hop_size, n_mels, mfcc=False):\n",
    "\n",
    "    input_data = Input(name='input', shape=(None,), dtype=\"float32\")\n",
    "    spec = LogMelgramLayer(\n",
    "        num_fft=fft_size,\n",
    "        hop_length=hop_size,\n",
    "        num_mels=n_mels,\n",
    "        sample_rate=sr,\n",
    "        f_min=0.0,\n",
    "        f_max=sr // 2,\n",
    "        eps=1e-6)(input_data)\n",
    "    x = BatchNormalization(axis=2)(spec)\n",
    "    # x = Permute((2, 1, 3), name='permute', dtype=\"float32\")(x)\n",
    "    model = Model(inputs=input_data, outputs=x, name=\"preprocessin_model\")\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### hop_size and  n_mels choise\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compare(i, fft_size, n_mels_list, hop_size_list, sr=16000):\n",
    "\n",
    "    sample_data = sample_generator.__getitem__(i)\n",
    "    sample_audios = sample_data[0][\"the_input\"]\n",
    "    sample_labels = sample_data[0][\"the_labels\"]\n",
    "\n",
    "    nrows, ncols = len(hop_size_list), len(n_mels_list),\n",
    "    plt.figure(figsize=(4 * nrows, 4 * ncols))\n",
    "\n",
    "    for i in range(nrows):\n",
    "        n_mels = n_mels_list[i]\n",
    "\n",
    "        for y in range(ncols):\n",
    "            hop_size = hop_size_list[y]\n",
    "\n",
    "            plt.subplot(nrows, ncols, i * ncols + y + 1)\n",
    "\n",
    "            model = preprocessin_model(fft_size, hop_size, n_mels)\n",
    "            pred = model.predict(sample_audios)\n",
    "\n",
    "            pred = pred[0, :, :, 0]\n",
    "            librosa.display.specshow(pred.T, sr=sr, hop_length=hop_size, cmap=\"jet\")\n",
    "            plt.title('hop: {}, n_mels: {}, shape: {}'.format(hop_size, n_mels, pred.shape), fontsize=11)\n",
    "\n",
    "    print(\"The longest sentence in this batch has {} characters\".format(sample_labels.shape[1]))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fft_size = 256\n",
    "n_mels_list = [256, 160, 128, 64]\n",
    "hop_size_list = [256, 160, 128, 64]\n",
    "compare(260, fft_size, n_mels_list, hop_size_list, sr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final Choice"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fft_size = 256\n",
    "hop_size = 128\n",
    "n_mels = 128\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "melspecModel = preprocessin_model(fft_size, hop_size, n_mels)\n",
    "melspecModel.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def vis(j=5):\n",
    "    for i in range(0, 220, 220 // j):\n",
    "        sample_data = sample_generator.__getitem__(i)\n",
    "        sample_audios = sample_data[0][\"the_input\"]\n",
    "        sample_labels = sample_data[0][\"the_labels\"]\n",
    "        sample_labels_length = sample_data[0][\"input_length\"]\n",
    "\n",
    "        melspec = melspecModel.predict(sample_audios)\n",
    "\n",
    "        print('\\n')\n",
    "        print('-' * 100)\n",
    "\n",
    "        print(\"The longest sentence in this batch has {} characters\".format(sample_labels.shape[1]))\n",
    "        print(\"We have to multiply the longest sentence by {} to reach length of Time steps\".format(\n",
    "            np.log2([melspec.shape[1] / sample_labels.shape[1]])[0]))\n",
    "\n",
    "        print('-' * 100)\n",
    "        print('\\n')\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 4))\n",
    "        pred = melspec[0, :, :, 0]\n",
    "        vis_model(pred, \"Mel-frequency spectrogram\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def vis_model(pred, title, cmap=\"jet\"):\n",
    "    librosa.display.specshow(pred.T, sr=sr, y_axis='mel', x_axis='time', hop_length=hop_size, cmap=cmap)\n",
    "    plt.title('{}. Shape = {}'.format(title, pred.shape))\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vis(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CTC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def input_lengths_lambda_func(args):\n",
    "    input_length = args\n",
    "    return tf.cast(tf.math.ceil(input_length / hop_size), dtype=\"float32\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = input_lengths_lambda_func(sample_audios_length[1]).numpy()\n",
    "x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def add_ctc_loss(model_builder):\n",
    "    the_labels = Input(name='the_labels', shape=(None,), dtype='float32')\n",
    "    input_lengths = Input(name='input_length', shape=(1,), dtype='float32')\n",
    "    label_lengths = Input(name='label_length', shape=(1,), dtype='float32')\n",
    "\n",
    "    input_lengths2 = Lambda(input_lengths_lambda_func)(input_lengths)\n",
    "    if model_builder.output_length:\n",
    "        output_lengths = Lambda(model_builder.output_length)(input_lengths2)\n",
    "    else:\n",
    "        output_lengths = input_lengths2\n",
    "\n",
    "    # CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
    "        [model_builder.output, the_labels, output_lengths, label_lengths])\n",
    "    model = Model(inputs=[model_builder.input, the_labels, input_lengths, label_lengths], outputs=loss_out)\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def simple_rnn_model(input_dim, output_dim=224):\n",
    "\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "\n",
    "    simp_rnn = GRU(output_dim, return_sequences=True, implementation=2, name='rnn')(input_data)\n",
    "\n",
    "    y_pred = Activation('softmax', name='softmax')(simp_rnn)\n",
    "\n",
    "    model = Model(inputs=input_data, outputs=y_pred, name=\"simple_rnn_model\")\n",
    "\n",
    "    model.output_length = lambda x: x\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "simple_rnn_model = simple_rnn_model(128, 224)\n",
    "plot_model(simple_rnn_model, to_file='../img/simple_rnn_model.png')\n",
    "simple_rnn_model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Builder\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_model(output_dim, custom_model, mfcc=False, calc=None):\n",
    "\n",
    "    input_audios = Input(name='the_input', shape=(None,))\n",
    "    pre = melspecModel(input_audios)\n",
    "    pre = tf.squeeze(pre, [3])\n",
    "\n",
    "    y_pred = custom_model(pre)\n",
    "    model = Model(inputs=input_audios, outputs=y_pred, name=\"model_builder\")\n",
    "    model.output_length = calc\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model0 = build_model(len(tokenizer.word_index) + 2, simple_rnn_model)\n",
    "model0.summary()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parameters\n",
    "batch_size = 32\n",
    "shuffle = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "split_point = int(data.shape[0] * .8)\n",
    "train_data = data[:split_point]\n",
    "val_data = data[split_point:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_gen = DataGenerator(train_data, sr, batch_size, False)\n",
    "val_gen = DataGenerator(val_data, sr, batch_size, False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(model_builder,\n",
    "          model_name,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          optimizer=SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),\n",
    "          ):\n",
    "\n",
    "    model = add_ctc_loss(model_builder)\n",
    "\n",
    "    # optimizer = Adam(lr=.01, clipnorm = 1, decay=1e-6)\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
    "    print(model.summary())\n",
    "\n",
    "    # make results/ directory, if necessary\n",
    "    if not os.path.exists('../models'):\n",
    "        os.makedirs('../models')\n",
    "\n",
    "    # add checkpointer\n",
    "    checkpointer = ModelCheckpoint(filepath=\"../models/\" + model_name + '.h5', verbose=0)\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "    # train the model\n",
    "    hist = model.fit_generator(generator=train_gen,\n",
    "                               validation_data=val_gen,\n",
    "                               epochs=epochs,\n",
    "                               callbacks=[checkpointer, early_stopping],\n",
    "                               verbose=verbose,\n",
    "                               use_multiprocessing=False)\n",
    "\n",
    "    # save model loss\n",
    "    with open(\"../models/\" + model_name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(hist.history, f)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train(model_builder=model0, model_name=\"simple_rnn_model\", epochs=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "simple_rnn_model = simple_rnn_model(sr, 12, fft_size, hop_size, n_mels)\n",
    "plot_model(simple_rnn_model, to_file='models/simple_rnn_model.png')\n",
    "simple_rnn_model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "a19a43a1a3446d00bf3142d3471365a00f919c4d4295bfac1b68bf8cb7a6fbbb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}