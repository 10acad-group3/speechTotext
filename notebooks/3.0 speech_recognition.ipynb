{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Speech Recognition\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, TensorBoard"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "sns.set()\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"expand_frame_repr\", False)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from clean_audio import CleanAudio\n",
    "from file_handler import FileHandler\n",
    "from audio_vis import AudioVis"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "clean_audio = CleanAudio()\n",
    "file_handler = FileHandler()\n",
    "audio_vis = AudioVis()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "PATH_TRAIN_WAV = \"../data/AMHARIC_CLEAN/train/wav/\"\n",
    "PATH_TEST_WAV = \"../data/AMHARIC_CLEAN/test/wav/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "data = pd.read_csv(r'../data/final_data.csv')\n",
    "data.sort_values(by=[\"duration\"], inplace=True)\n",
    "data.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Unnamed: 0               key                  text  char_length  duration  speed category\n",
       "5402         5655   tr_5302_tr54003         ሼራተን አዲስ ተመረቀ           13      2.05   6.35    Train\n",
       "10003       10511  tr_9674_tr094116        ፖሊሱ እስረኞቹን ቆጠረ           14      2.05   6.84    Train\n",
       "6674         7012   tr_6524_tr66025     ይህ ቀስ በቀስ እያደገ ሄደ           17      2.05   8.30    Train\n",
       "8285         8686   tr_8030_tr81031     ኢነጋማ ህጋዊ እውቅና አገኘ           17      2.05   8.30    Train\n",
       "6017         6314   tr_5897_tr59098  በተጨባጭ ስና የው ግን ባዶ ነው           20      2.05   9.77    Train"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>key</th>\n",
       "      <th>text</th>\n",
       "      <th>char_length</th>\n",
       "      <th>duration</th>\n",
       "      <th>speed</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5402</th>\n",
       "      <td>5655</td>\n",
       "      <td>tr_5302_tr54003</td>\n",
       "      <td>ሼራተን አዲስ ተመረቀ</td>\n",
       "      <td>13</td>\n",
       "      <td>2.05</td>\n",
       "      <td>6.35</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>10511</td>\n",
       "      <td>tr_9674_tr094116</td>\n",
       "      <td>ፖሊሱ እስረኞቹን ቆጠረ</td>\n",
       "      <td>14</td>\n",
       "      <td>2.05</td>\n",
       "      <td>6.84</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6674</th>\n",
       "      <td>7012</td>\n",
       "      <td>tr_6524_tr66025</td>\n",
       "      <td>ይህ ቀስ በቀስ እያደገ ሄደ</td>\n",
       "      <td>17</td>\n",
       "      <td>2.05</td>\n",
       "      <td>8.30</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8285</th>\n",
       "      <td>8686</td>\n",
       "      <td>tr_8030_tr81031</td>\n",
       "      <td>ኢነጋማ ህጋዊ እውቅና አገኘ</td>\n",
       "      <td>17</td>\n",
       "      <td>2.05</td>\n",
       "      <td>8.30</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6017</th>\n",
       "      <td>6314</td>\n",
       "      <td>tr_5897_tr59098</td>\n",
       "      <td>በተጨባጭ ስና የው ግን ባዶ ነው</td>\n",
       "      <td>20</td>\n",
       "      <td>2.05</td>\n",
       "      <td>9.77</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    def __init__(self, texts, padding, len_sent, filters, reverse=False):\n",
    "        Tokenizer.__init__(self, filters=filters, char_level=True)\n",
    "\n",
    "        self.len_sent = len_sent\n",
    "        self.fit_on_texts(texts)\n",
    "\n",
    "        self.index_to_word = dict(zip(self.word_index.values(), self.word_index.keys()))\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "\n",
    "        if reverse:\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen=len_sent,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating\n",
    "                                           )\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word\n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        words = [self.index_to_word[token] for token in tokens if token != 0]\n",
    "        text = \"\".join(words)\n",
    "        return text\n",
    "\n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "\n",
    "        if padding:\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen=self.len_sent,\n",
    "                                   padding=truncating,\n",
    "                                   truncating=truncating\n",
    "                                   )\n",
    "        return tokens\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "MAX_SENTENCE_LENGTH = 125       # The longest sentence in the data is around 150 chars\n",
    "filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n።”፤፦’፥'  # { ።”፤፦’፥' } unique for amharic"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "%%time\n",
    "tokenizer = TokenizerWrap(texts=data.text,\n",
    "                          padding='post',\n",
    "                          reverse=False,\n",
    "                          len_sent=MAX_SENTENCE_LENGTH,\n",
    "                          filters=filters)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 543 ms, sys: 22.6 ms, total: 566 ms\n",
      "Wall time: 618 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "print(len(tokenizer.word_index))\n",
    "print(tokenizer.word_index)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "222\n",
      "{' ': 1, 'ን': 2, 'ት': 3, 'ው': 4, 'ስ': 5, 'ያ': 6, 'የ': 7, 'ተ': 8, 'በ': 9, 'አ': 10, 'ል': 11, 'እ': 12, 'ለ': 13, 'ር': 14, 'መ': 15, 'ም': 16, 'ች': 17, 'ና': 18, 'ደ': 19, 'ነ': 20, 'ገ': 21, 'ማ': 22, 'ባ': 23, 'ይ': 24, 'ሚ': 25, 'ግ': 26, 'ራ': 27, 'ቸ': 28, 'ላ': 29, 'ብ': 30, 'ድ': 31, 'ረ': 32, 'ሰ': 33, 'ከ': 34, 'ወ': 35, 'ኢ': 36, 'ታ': 37, 'ዳ': 38, 'ክ': 39, 'ዮ': 40, 'ዋ': 41, 'ህ': 42, 'ጵ': 43, 'ጥ': 44, 'ቀ': 45, 'ሪ': 46, 'ጠ': 47, 'ቅ': 48, 'ዲ': 49, 'ሳ': 50, 'ዎ': 51, 'ሮ': 52, 'ሩ': 53, 'ሉ': 54, 'ሆ': 55, 'ሁ': 56, 'ጋ': 57, 'ሊ': 58, 'ቶ': 59, 'ካ': 60, 'ፈ': 61, 'ጣ': 62, 'ፍ': 63, 'ሀ': 64, 'ሞ': 65, 'ሽ': 66, 'ዊ': 67, 'ዘ': 68, 'ቱ': 69, 'ሬ': 70, 'ኤ': 71, 'ኮ': 72, 'ሎ': 73, 'ኛ': 74, 'ዛ': 75, 'ሲ': 76, 'ቃ': 77, 'ጉ': 78, 'ቡ': 79, 'ቻ': 80, 'ዝ': 81, 'ፕ': 82, 'ቢ': 83, 'ዚ': 84, 'ኑ': 85, 'ሙ': 86, 'ሶ': 87, 'ጀ': 88, 'ቁ': 89, 'ኖ': 90, 'ኩ': 91, 'ቋ': 92, 'ሌ': 93, 'ቤ': 94, 'ሱ': 95, 'ኒ': 96, 'ቹ': 97, 'ኝ': 98, 'ጸ': 99, 'ዱ': 100, 'ቲ': 101, 'ጅ': 102, 'ሸ': 103, 'ዜ': 104, 'ቴ': 105, 'ቆ': 106, 'ዙ': 107, 'ዴ': 108, 'ኔ': 109, 'ጡ': 110, 'ኙ': 111, 'ፋ': 112, 'ዶ': 113, 'ጐ': 114, 'ጫ': 115, 'ፊ': 116, 'ጽ': 117, 'ጃ': 118, 'ጊ': 119, 'ጻ': 120, 'ኞ': 121, 'ሄ': 122, 'ጨ': 123, 'ቦ': 124, 'ሴ': 125, 'ዩ': 126, 'ኦ': 127, 'ፖ': 128, 'ሻ': 129, 'ሜ': 130, 'ፓ': 131, 'ጭ': 132, 'ኳ': 133, 'ኘ': 134, 'ቷ': 135, 'ጂ': 136, 'ጓ': 137, 'ቂ': 138, 'ፌ': 139, 'ፉ': 140, 'ኬ': 141, 'ኪ': 142, 'ጦ': 143, 'ዬ': 144, 'ሏ': 145, 'ሺ': 146, 'ፒ': 147, 'ፎ': 148, 'ጆ': 149, 'ሂ': 150, 'ቾ': 151, 'ቨ': 152, 'ጹ': 153, 'ሟ': 154, 'ጤ': 155, 'ሹ': 156, 'ጮ': 157, 'ዞ': 158, 'ቄ': 159, 'ጪ': 160, 'ሯ': 161, 'ኗ': 162, 'ኋ': 163, 'ጄ': 164, 'ቧ': 165, 'ሷ': 166, 'ኡ': 167, 'ሾ': 168, 'ጿ': 169, 'ቪ': 170, 'ጁ': 171, 'ኰ': 172, 'ፏ': 173, 'ዥ': 174, 'ጳ': 175, 'ጌ': 176, 'ጧ': 177, 'ፐ': 178, 'ኸ': 179, 'ፔ': 180, 'ቬ': 181, 'ዷ': 182, 'ዪ': 183, 'ቼ': 184, 'ቺ': 185, 'ጩ': 186, 'ዟ': 187, 'ሼ': 188, 'ቫ': 189, 'ዤ': 190, 'ዌ': 191, 'ጾ': 192, 'ቭ': 193, 'ኟ': 194, 'ሿ': 195, 'ጼ': 196, 'ኜ': 197, 'ጬ': 198, 'ዢ': 199, 'ጯ': 200, 'ጺ': 201, 'ፑ': 202, 'ጶ': 203, 'ዡ': 204, 'ጢ': 205, 'ኚ': 206, 'ቿ': 207, 'ጇ': 208, 'ጴ': 209, 'ዉ': 210, 'ጰ': 211, 'ዧ': 212, 'ጔ': 213, 'ዣ': 214, 'ዦ': 215, 'ዠ': 216, 'ቩ': 217, 'ቮ': 218, 'ኲ': 219, 'ጲ': 220, 'ጱ': 221, 'ቯ': 222}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "data.text[1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'የጠመንጃ ተኩስ ተከፈተና አራት የኤርትራ ወታደሮች ተገደሉ'"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "sample = tokenizer.text_to_tokens(data.text[1], padding=True)\n",
    "sample"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[  7,  47,  15,   2, 118,   1,   8,  91,   5,   1,   8,  34,  61,\n",
       "          8,  18,   1,  10,  27,   3,   1,   7,  71,  14,   3,  27,   1,\n",
       "         35,  37,  19,  52,  17,   1,   8,  21,  19,  54,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0]], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "print(tokenizer.tokens_to_string(sample[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "የጠመንጃ ተኩስ ተከፈተና አራት የኤርትራ ወታደሮች ተገደሉ\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "save token"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "with open('../models/char_tokenizer_amharic.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g9/42gvsspd50xb9dstjnwqm1wr0000gn/T/ipykernel_17630/1115101889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/char_tokenizer_amharic.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "a19a43a1a3446d00bf3142d3471365a00f919c4d4295bfac1b68bf8cb7a6fbbb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}